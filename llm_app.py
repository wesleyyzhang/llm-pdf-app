# -*- coding: utf-8 -*-
"""Yet another copy of RAG_LLM_PDF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rpNX5pjhchnw3uvudwMXIABCQs3tRH5N
"""

import streamlit as st
from llama_index.core import VectorStoreIndex, Settings, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM
from tempfile import TemporaryDirectory
import os

# Initialize LLM + embedding once
@st.cache_resource
def init_llm_and_embedding():
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en")
    llm = HuggingFaceLLM(
        model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        tokenizer_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        context_window=2048,
        max_new_tokens=256,
        generate_kwargs={"do_sample": False},
        device_map="auto"
    )
    Settings.embed_model = embed_model
    Settings.llm = llm
    Settings.chunk_size = 256
    Settings.chunk_overlap = 0
    return embed_model, llm

embed_model, llm = init_llm_and_embedding()

st.title("ðŸ“„ RAG PDF Q&A App")

uploaded_files = st.file_uploader("Upload one or more PDFs", type="pdf", accept_multiple_files=True)

if uploaded_files:
    with TemporaryDirectory() as tmpdir:
        for file in uploaded_files:
            file_path = os.path.join(tmpdir, file.name)
            with open(file_path, "wb") as f:
                f.write(file.read())

        st.success("PDFs uploaded successfully! Building index...")

        # Load & index PDFs
        documents = SimpleDirectoryReader(tmpdir).load_data()
        index = VectorStoreIndex.from_documents(documents)
        query_engine = index.as_query_engine()

        st.success("Index built! You can now ask questions.")

        # User query
        query = st.text_input("Ask a question about the uploaded PDFs:")
        if query:
            with st.spinner("Thinking..."):
                response = query_engine.query(query)
            st.markdown(f"**Answer:** {response}")

